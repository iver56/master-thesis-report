% !TEX encoding = UTF-8 Unicode
%!TEX root = thesis.tex
% !TEX spellcheck = en-US
%%=========================================
\chapter{Methods and Implementation}
\label{chapter:methods_and_implementation}

\section{Evolving Neural Networks}
There are several ways to map sound analysis to effect parameters. The initial idea was a modulation matrix, where each effect parameter becomes a linear combination of the audio features. This idea was quickly iterated upon. Why not use an artificial neural network? An artificial neural network can do the same as a modulation matrix (when the ANN has linear activation functions and no hidden layers), but it can also do more. A neural network can have hidden nodes with various activation functions, which makes more complex signal interactions possible. This widens the scope, and allows for learning higher-level features, such as “the snare and the bass drum are hit simultaneously”.

NEAT \citep{stanley2002} is suggested as a technique for evolving neural networks. This is a technique which evolves not only the weights of the neural network, but also the topology, i.e. the number of nodes and the connections between the nodes. HyperNEAT, a technique that is based on NEAT, has been used with great success in a project called Picbreeder \citep{secretan2008} and later with some success in a project called SoundBreeder \citep{ye2014}. The purpose of those projects were to evolve visually and aurally appealing art, respectively. Since both those two projects and this project are within the field of creative computation, the hypothesis is that NEAT or HyperNEAT will work well in this project.

\section{Implementation}
Csound has been selected to be the audio processing tool in this project because it is actively used by students and lecturers in the Music Technology department at NTNU. Since Csound interoperates nicely with Python, and Python is a popular language for artificial intelligence (AI) applications, that was the language of choice for the neuroevolution application. Further, it has been decided that the application should be written in a style compatible with both Python 2 and 3, for the sake of compatibility with various Python libraries. Also, it should work on both Windows and Ubuntu. This way, experiments can not only run on Windows desktop computers, but also on Ubuntu instances in the cloud. The most important dependency is MultiNEAT \citep{multineat}, which is one of Kenneth Stanley’s recommended neuroevolution libraries \citep{neatsoftware}. It features several neuroevolution algorithms, including NEAT, FS-NEAT and HyperNEAT. It is written in C++, but it has Python bindings, so it is fit for this project. Further, the audio features extraction tools selected for this project are Aubio, Essentia and LibXtract, which are all free open source software written in a compiled language. A link to the author’s implementation of the toolkit, which is open source, is included in appendix \ref{appendix:toolkit}. A complete list of dependencies is included in appendix \ref{appendix:python_dependencies}.

\subsection{Performance}
A. Eldhuset has previously implemented a program that uses Csound in signal interaction experiments with genetic algorithms. He concluded that his implementation was slow, taking around 5 seconds per individual \citep{eldhuset2015}. Hence an experiment with a population size of 20 and 20 generations would take approximately half an hour. The author has analyzed the weaknesses of Eldhuset's approach and come up with a number of techniques to alleviate performance issues:

\begin{itemize}  
\item Use a templating engine to generate Csound files. It writes csound code to different files, one for each individual in the population. This allows many Csound instances to be run in parallel.
\item While a Csound instance runs, it does not have to communicate with another program (a host) via an API. All data needed for the run, including effect parameter values over time, are included inside the csd file
\item Use dedicated, compiled audio feature extraction tools such as Aubio instead of Csound for audio feature extraction
\item Use the standard streams (stdout) instead of file I/O in audio feature extractors that support this
\item Let the host program, Csound and audio feature extractors write files to a RAM disk to avoid slow disk I/O activity
\item Use the concept of pipelining to shorten critical paths and enable more parallelism
\item Sensible handling of duplicate individuals: when two or more individuals are equal (i.e. their neural networks are equal), evaluate only one of them, and apply the same result to the identical individuals
\end{itemize}

Incorporating all these techniques, the author's implementation spends around 0.11 seconds on average per individual, given that the dist\_lpf effect and the aubio mfcc analyzer is used, the duration of the input sound is 7 seconds, and that the program is run on a modern, high-end laptop with two CPU cores. Hence an experiment with population size 20 and 20 generations may take approximately half a minute. If the input sound is shorter, the experiment will run even faster. This is exploited in Experiment 1, where over a million individuals are processed, so fast performance is really useful.

\subsection{Neuroevolution Routine}
The neuroevolution program is called from the command line, with a number of arguments for configuring the experiment. The program then performs roughly these steps:

\begin{enumerate}  
\item Check sanity of arguments
\item Analyze input sound file and target sound file
\item Initialize a population
\item For each generation, evaluate all individuals, write their data to json files and then advance to the next generation
\end{enumerate}

The evaluation of an individual (illustrated by figure \ref{fig:individual_evaluation_process}) involves several operations:

\begin{enumerate}  
\item An artificial neural network is created from the genotype of the individual
\item All feature vectors of the target sound are run through the neural network
\item The neural outputs are scaled to appropriate ranges for the various audio effect parameters
\item Csound runs the input sound through the audio effect that is controlled by the audio effect parameters
\item The resulting sound is run through the audio feature extraction tool(s)
\item The audio features are standardized with the same mean and variance as in the standardization of the target sound audio features
\item The audio features of the target sound and the output sound are used in the fitness function
\item The resulting fitness value is assigned to the individual.
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{06_individual_evaluation_process}
    \caption{Flowchart for the evaluation of an individual}
    \label{fig:individual_evaluation_process}
\end{figure}

\subsection{Input Standardization}
To make the audio features suitable as input to a neural network, they need to be scaled. A simple, but good technique is to standardize them by is subtracting the mean and dividing by the standard deviation \citep{sarle2014}. For each audio feature there is one sequence of numbers for the input sound and another sequence for the target sound. The mean and standard deviation for an audio feature is calculated from the input sound’s sequence for that feature concatenated with the target sound’s sequence for that feature. This mean and standard deviation is then used in all further input standardization. This gives the series the quality of being centered around zero and having a standard deviation of 1 with respect to the input sound and the target sound. Additionally, to avoid extreme values, values are clipped to the range [-4, 4].

\subsection{Parameter Mapping}
The output of the neural network is in the range [0, 1], due to the sigmoid activation function in the output layer. These normalized values need to be scaled and skewed appropriately for each effect parameter. The author has used the same mapping function as in Cabbage \citep{walsh2008}, a GUI framework for Csound, where slider values are mapped from 0 to 1 to the target range by the following function:

$$f(x)=m_{min}+(m_{max}-m_{min})*e^{\log(x)/s}$$

Where $m_{min}$ and $m_{max}$ are the endpoints of the target range and s is the skew factor. The default skew factor is 1, which will yield a linear mapping. A skew factor of 0.5 will cause the mapping function to output values in an exponential fashion. This is useful for effect parameters such as cutoff frequency.

\subsection{Audio Effects}
In the toolkit, audio effects are included as text files containing Csound code. Each effect also has a corresponding file that lists the parameters of the effect and their range. This makes it easy to add new Csound effects to the toolkit. The toolkit uses a templating system for generating Csound files, and this makes it possible to combine several audio effects into one larger audio effect with one or more layer of parallel audio effects, as tested in experiment 5. The implementation of the various audio effects used in this toolkit are based on trusted sources: The distortion effect with resonant low-pass filter is based on the source code of \cite{brandtsegg2015}\footnote{\url{https://github.com/Oeyvind/interprocessing}} while the rest is based on source code of NTNUs online course in Digital Signal Processing (DSP) ear training\footnote{\url{https://github.com/gdsp/gdsp/}}. These implementations have been selected in consultation with the author's co-supervisor, Brandtsegg.

\subsection{Data Augmentation}
Data augmentation is implemented as a python module that takes in a sound and creates a Csound file that repeats the sound multiple times with some variations and outputs the resulting audio to a new file. Each repetition of the sound has a unique combination of gain and playback speed. The playback speed and gain are sampled from a gaussian distribution centered around $1$. The standard deviation of this gaussian distribution is configurable. To prevent clipping (samples out of bounds due to high gain values), a limiter is applied. Other ways to augment sound, such as adding reverb or distortion, can be easily implemented.

\subsection{Live Mode}
The toolkit includes a python module that can create a Csound file from an individual in an experiment. This Csound file includes an audio analyzer and a neural network that can run on live audio streams, so it can be used in live performances. It can also run in offline mode, to speed up computation when live mode is not needed. This is used in experiment 3 to apply evolved cross-adaptive audio effects to unseen data (i.e. sounds that were not used during training).

\section{Visualization}
In very early versions of the neuroevolution program, the author found it hard to evaluate all the data produced during experiments. Therefore an interactive web application for visualizing results was developed. This tool has been very important for being able to understand the strengths and weaknesses of the neuroevolution program during development and research. When the author gained a good understanding of the inner workings and the output of the neuroevolution program, he was able to improve the weak points of the implementation.

The visualization system is a single-page web application written in AngularJS, with various JavaScript libraries for visualizing data. For a complete list of JavaScript libraries that were used in the web application, see appendix \ref{appendix:javascript_dependencies}. The application server is written in NodeJS. The neuroevolution program writes data after each generation, and the NodeJS server listens for these data updates. Whenever new data is available, the updated data is sent via WebSockets to the web application, which then updates its views. The four following figures are screenshots of the web application. Figure \ref{fig:viz_fitness_plot} shows a line chart that visualizes the progress of the GA over the generations. Figure \ref{fig:viz_species_plot} shows a stacked area chart that visualizes the number of individual in each NEAT species over the generations. The fitness histogram in figure \ref{fig:viz_fitness_histogram} visualizes the fitness distribution in the population of the generation selected by the interactive slider.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{viz_fitness_plot}
    \caption{Fitness plot}
    \label{fig:viz_fitness_plot}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{viz_species_plot}
    \caption{Species plot}
    \label{fig:viz_species_plot}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{viz_fitness_histogram}
    \caption{Fitness histogram for the selected generation}
    \label{fig:viz_fitness_histogram}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.88\textwidth]{viz_individual}
    \caption{Visualization of data about an individual}
    \label{fig:viz_individual}
\end{figure}

Figure \ref{fig:viz_individual} shows various data visualizations related to the individual selected by the interactive slider. Here are brief explanations of the different parts:
\begin{itemize}
\item A: The interactive slider is for selecting a specific individual in the population of the selected generation. The interface below it updates whenever a different individual is selected.
\item B: Neural network visualization with edge colors according to weight. Light color means small magnitude while strong color means large magnitude. Red means positive weight while blue means negative. It is possible to click a node to see a list of exact values for ingoing and outgoing weights.
\item C: Select which sound and corresponding data series to visualize.
\item D: Audio player with waveform visualization. Useful for playing back the output sound of the selected individual.
\item E and F: Horizon charts for visualizing neural input and neural output, respectively. Horizon charts make better use of vertical space than standard area charts, allowing one to see many more metrics at-a-glance. Larger values are overplotted in successively darker colors, while negative values are offset to descend from the top. When hovering over these charts with the mouse pointer, series labels disappear and a rule with values at the hovered time step is shown. The horizon charts are aligned with the audio player.
\item G: Shows the effect(s) and its parameters. While playing the audio, the knob positions are animated according to the way they were controlled in order to produce the output sound.
\end{itemize}

