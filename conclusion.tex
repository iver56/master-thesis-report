% !TEX encoding = UTF-8 Unicode
%!TEX root = thesis.tex
% !TEX spellcheck = en-US
%%=========================================
\chapter{Conclusion}
\label{chapter:conclusion}

It has been shown that using neuroevolution for finding useful mappings in cross-adaptive audio effects is feasible. This is good because finding these signal mappings by empirical experimentation can be tedious and hard due to the vast number of combinations.

Several fitness functions have been developed and compared. Based on qualitative evaluations, the hybrid variant, that is a combination of local euclidean distance and NSGA-II-inspired multi-objective optimization, has been found to yield the best results. Furthermore, in experiments with high-dimensional spaces, FS-NEAT has been proven to do better than NEAT, because FS-NEAT chooses only a few useful connections rather than a fully connected neural network.

A comprehensive toolkit has been developed during the course of the project. The toolkit includes an interactive visualization tool that makes it possible to evaluate results and understand the neuroevolution process. The toolkit has lots of configuration options, enabling a flexible platform for experimentation. It is open source, has documentation and can be used in future research within the field of cross-adaptive audio effects.

While an evolved cross-adaptive audio effect may perform well on the combination of the input sound and the target sound it was trained on, it is also desirable to be able to successfully apply the effect to other sounds. In particular, this is useful in live performances where audio effects are applied to unseen sound. When training data is scarce, data augmentation can be applied to evolve cross-adaptive audio effects that perform better on unseen sound that deviates from the training sound. Audio effects produced by the toolkit can be used in live performances, thanks to the implementation of audio analysis, audio effects and artificial neural network that can run on live audio streams in Csound.

\section{Future Work}

As stated in the introduction, current research at the Music Technology department at Norwegian University of Science and Technology aims at exploring radically new modes of musical interaction in live music performance. This project is a good start, but there is still a lot to be explored. For example, it would be interesting to try other audio effects than the six audio effects used in this project. Also, it does not have to be effects. It can also be sound generators, such as synthesizers or sword sound emulators, with parameters. For example, a foley artist can imitate a sword sound with his mouth, then evolve parameters that make the sword sound generator produce a sound like that. This relates to the work of \cite{cartwright2014}. While their project is based on interactive, iterative refinement through user-provided relevance feedback, neuroevolution can automate that process to save time. That could make foley sound production easier and less time-consuming.

More work can also be done on combining multiple audio effects into one composite audio effect. In experiment 5, layers of parallel audio effects were tested, and the results were not better than the best effect used individually. However, this does not mean that composite effects are generally bad. There are lots of techniques to be explored that might improve composite effects. For example, one could use Cartesian Genetic Programming (CGP) to automatically evolve the topology of effect networks.

Picbreeder \citep{secretan2008} and Soundbreeder \citep{ye2014} had success with HyperNEAT, which is one variant of NEAT that has not been tried in this project. HyperNEAT might be able to produce better results than NEAT in this project, but probably only in experiments where the input nodes and output nodes have some sort of geometrical meaning \citep{whiteson2013}.

While live mode is implemented and technically works, it has not been tried by music performers yet. There is much work to be done on finding the role of evolved cross-adaptive audio effects in live performances. For example, one has to find out which instruments interact with each other's cross-adaptive effects and which audio effects are musically interesting and appealing. Other potential issues, such as latency and audio feedback, also need to be dealt with.

The author imagines that methods developed in this project could be used for mastering/mixing music and also for novel crossfading in DJ mixing software. However, that would require smart methods for dealing with long sounds (several minutes). This project has only dealt with short sounds (up to 16 seconds) so far. When dealing with longer sounds the author sees two challenges: 1) Computational time and 2) A long sound might have several very different parts, and the evolved neural network might have trouble dealing well with all of them. One possible solution to these challenges is to chop the long sound into a few short audio segments that represent the different parts of the sound well and then run the program on each audio segment. When applying the resulting neural networks on new sounds, the program can automatically fade between the evolved artificial neural networks based on similarity with the various audio segments they were trained on.
